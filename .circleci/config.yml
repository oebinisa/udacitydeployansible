version: 2.1

# Use a package of configuration called an orb.
orbs:
  # Choose either one of the orbs below
  # welcome: circleci/welcome-orb@0.4.1
  aws-cli: circleci/aws-cli@2.0.3

commands:
  destroy_environment:
    steps:
      - run:
          name: Destroy environment
          # ${CIRCLE_WORKFLOW_ID} is a Built-in environment variable
          # ${CIRCLE_WORKFLOW_ID:0:5} takes the first 5 chars of the variable CIRCLE_CI_WORKFLOW_ID
          when: on_fail
          command: |
            aws cloudformation delete-stack \
            --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:18}

# Define the jobs we want to run for this project
jobs:
  create_infrastructure: # Job 1
    docker: # The primary container, where your job's commands will run
      - image: amazon/aws-cli # docker image with AWS CLI installed
    steps:
      - checkout # check out the code in the project directory
      - run:
          name: Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
              --template-file template.yml \
              --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:18} \
              --region us-west-2
      - run:
          name: create_ansible_inventory_file_with_header
          command: |
            echo "[all]" > inventory
      - run:
          name: add_ec2_instances_ipv4_to_ansible_inventory_file
          command: |
            aws ec2 describe-instances \
            --query 'Reservations[*].Instances[*].PublicIpAddress' \
            --output text >> inventory
      # Fail the job intentionally to simulate an error.
      # Uncomment the line below if you want to fail the current step
      # - run: return 1
      - destroy_environment

  configure_infrastructure: # Job 2 - Config and Deployment
    docker: # The primary container, where your job's commands will run
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - add_ssh_keys:
          fingerprints: [f4:cc:19:75:f8:9c:0d:b5:70:b8:db:68:8c:72:e2:3f]
      - run:
          name: Install Ansible # Install Ansible
          command: |
            apk add --update ansible
      - run:
          name: Run Playbook and Configure server
          command: |
            ansible-playbook -i inventory main-remote.yml

  smoke_test: # Job 3 - Smoke Testing
    docker:
      - image: alpine:latest
    steps:
      - run:
          name: Update the Curl package
          command: |
            apk add --update curl
      - run:
          name: Smoke test job
          command: |
            URL="https://blog.udacity.com/"
            # Test if website exists
            if curl -s --head ${URL}
            then
              return 0
            else
              return 1
            fi
      - destroy_environment

  #cloudfront: # Job Executed in 6 (promote_to_production)
  #  docker:
  #    - image: amazon/aws-cli # docker image with AWS CLI installed
  #  steps:
  #    - checkout # check out the code in the project directory
  #    - run:
  #        name: Create Cloudfront Distribution
  #        command: |
  #          aws cloudformation deploy \
  #            --template-file cloudfront.yml \
  #            --stack-name production-distro \
  #            --parameter-overrides PipelineID="${mypipelinebucket01}" \
  #            --tags project=udapeople &
  #    # Fail the job intentionally to simulate an error.
  #    # Uncomment the line below if you want to fail the current step
  #    # - run: return 1
  #    - destroy_environment

  # Exercise: Promote to Production:
  # Prerequisite:
  # 1. An S3 bucket (say `mybucket644752792305`) with a sample index.html created manually in your AWS console.
  # 2. Enable the Static website hosting in that bucket.
  # 3. Run the command below to create a CloudFront Distribution that will connect to the existing bucket.
  # Note that the `--parameter-overrides` let you specify a value that override parameter value in the cloudfront.yml template file.
  # We are assuming that the `PipelineID` parameter represents the bucket ID.

  # Executes the bucket.yml - Deploy an S3 bucket, and interface with that bucket to synchronize the files
  # between local and the bucket.
  # Note that the `--parameter-overrides` let you specify a value that override parameter value in the bucket.
  # yml template file.
  create_and_deploy_front_end: # Job 4
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Execute bucket.yml - Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
            --template-file bucket.yml \
            --stack-name stack-create-bucket-${CIRCLE_WORKFLOW_ID:0:7} \
            --parameter-overrides MyBucketName="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"
      # Uncomment the step below if you wish to upload all contents of the current directory to the S3 bucket
      - run: aws s3 sync . s3://mybucket-${CIRCLE_WORKFLOW_ID:0:7} --delete

  # Fetch and save the pipeline ID (bucket ID) responsible for the last release.
  get_last_deployment_id: # Job 5
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - run:
          name: Fetch and save the old pipeline ID (bucket name) responsible for the last release.
          command: |
            aws cloudformation \
            list-exports --query "Exports[?Name==\`PipelineID\`].Value" \
            --no-paginate --output text > ~/textfile.txt
      - persist_to_workspace:
          root: ~/
          paths:
            - textfile.txt

  # Executes the cloudfront.yml template that will modify the existing CloudFront Distribution, change its
  # target from the old bucket to the new bucket - `mybucket-${CIRCLE_WORKFLOW_ID:0:7}`.
  # Notice here we use the stack name `production-distro` which is the same name we used while deploying to
  # the S3 bucket manually.
  promote_to_production: # Job 6
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Execute cloudfront.yml
          command: |
            aws cloudformation deploy \
            --template-file cloudfront.yml \
            --stack-name production-distro \
            --parameter-overrides PipelineID="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"

  # Destroy the previous production version's S3 bucket and CloudFormation stack.
  clean_up_old_front_end: # Job 7
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run: yum install -y tar gzip
      - attach_workspace:
          at: ~/
      - run:
          name: Destroy the previous S3 bucket and CloudFormation stack.
          # Use $OldBucketID environment variable or mybucket644752792305 below.
          # Similarly, you can create and use $OldStackID environment variable in place of production-distro
          command: |
            export OldBucketID=$(cat ~/textfile.txt)
            aws s3 rm "s3://${OldBucketID}" --recursive

# Sequential workflow
workflows:
  # Name the workflow
  myWorkflow:
    jobs:
      - create_infrastructure
      - configure_infrastructure:
          requires:
            - create_infrastructure
      - smoke_test:
          requires:
            - configure_infrastructure
      - create_and_deploy_front_end:
          requires:
            - smoke_test
      - promote_to_production:
          requires:
            - create_and_deploy_front_end
      - get_last_deployment_id:
          requires:
            - create_and_deploy_front_end
      - clean_up_old_front_end:
        requires:
          - get_last_deployment_id
          - promote_to_production
